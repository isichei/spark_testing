{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!conda install --yes --prefix {sys.prefix} boto3\n",
    "!{sys.executable} -m pip install git+git://github.com/moj-analytical-services/etl_manager.git#egg=etl_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove metastore and data used in this example (Want to start a blank slate)\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "if os.path.isdir('metastore_db') :\n",
    "    shutil.rmtree('metastore_db')\n",
    "\n",
    "if os.path.isfile('derby.log') :\n",
    "    os.remove('derby.log')\n",
    "    \n",
    "if os.path.isdir('data/hive_df') :\n",
    "    shutil.rmtree('hive_df')\n",
    "\n",
    "if os.path.isdir('data/non_hive_df') :\n",
    "    shutil.rmtree('non_hive_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jovyan/Documents/projects/spark_testing'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Attempt to combine the two\n",
    "\n",
    "import boto3\n",
    "import os\n",
    "# from dataengineeringutils.utils import read_json\n",
    "\n",
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "\n",
    "# Import own function library\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.amazonaws:aws-java-sdk:1.10.34,org.apache.hadoop:hadoop-aws:2.6.0 pyspark-shell'\n",
    "\n",
    "session = boto3.Session()\n",
    "credentials = session.get_credentials()\n",
    "\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"hive.exec.dynamic.partition\", \"true\") \\\n",
    "    .config(\"hive.exec.dynamic.partition.mode\", \"nonstrict\") \\\n",
    "    .config(\"spark.hadoop.fs.s3.impl\", \"org.apache.hadoop.fs.s3native.NativeS3FileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3.awsAccessKeyId\", credentials.access_key) \\\n",
    "    .config(\"spark.hadoop.fs.s3.awsSecretAccessKey\", credentials.secret_key) \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple example of overwrite differences\n",
    "\n",
    "Differences between HIVE and normal table in spark\n",
    "\n",
    "Some useful resources:\n",
    "\n",
    "- https://docs.databricks.com/spark/latest/spark-sql/language-manual/insert.html\n",
    "- https://medium.com/@anuvrat/writing-into-dynamic-partitions-using-spark-2e2b818a007a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+-------------+\n",
      "|col1|       col2|partition_bin|\n",
      "+----+-----------+-------------+\n",
      "|   0|init_record|            0|\n",
      "|   0|init_record|            0|\n",
      "|   0|init_record|            0|\n",
      "|   0| new_record|            1|\n",
      "|   0| new_record|            1|\n",
      "+----+-----------+-------------+\n",
      "\n",
      "+----+----------+-------------+\n",
      "|col1|      col2|partition_bin|\n",
      "+----+----------+-------------+\n",
      "|   0|new_record|            1|\n",
      "+----+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### CREATE HIVE TABLE (with one row)\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS hive_df (col1 INT, col2 STRING, partition_bin INT)\n",
    "USING HIVE OPTIONS(fileFormat 'PARQUET')\n",
    "PARTITIONED BY (partition_bin)\n",
    "LOCATION 'data/hive_df'\n",
    "\"\"\")\n",
    "spark.sql(\"\"\"\n",
    "INSERT INTO hive_df PARTITION (partition_bin = 0)\n",
    "VALUES (0, 'init_record')\n",
    "\"\"\")\n",
    "###\n",
    "\n",
    "### CREATE NON HIVE TABLE (with one row)\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS non_hive_df (col1 INT, col2 STRING, partition_bin INT)\n",
    "USING PARQUET\n",
    "PARTITIONED BY (partition_bin)\n",
    "LOCATION 'data/non_hive_df'\n",
    "\"\"\")\n",
    "spark.sql(\"\"\"\n",
    "INSERT INTO non_hive_df PARTITION (partition_bin = 0)\n",
    "VALUES (0, 'init_record')\n",
    "\"\"\")\n",
    "###\n",
    "\n",
    "### ATTEMPT DYNAMIC OVERWRITE WITH EACH TABLE\n",
    "spark.sql(\"\"\"\n",
    "INSERT OVERWRITE TABLE hive_df PARTITION (partition_bin)\n",
    "VALUES (0, 'new_record', 1)\n",
    "\"\"\")\n",
    "spark.sql(\"\"\"\n",
    "INSERT OVERWRITE TABLE non_hive_df PARTITION (partition_bin)\n",
    "VALUES (0, 'new_record', 1)\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"SELECT * FROM hive_df\").show() # 2 row dynamic overwrite\n",
    "spark.sql(\"SELECT * FROM non_hive_df\").show() # 1 row full table overwrite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More messing about "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test new update method\n",
    "df_partitions = ['partition_bin']\n",
    "in_schema = StructType([StructField('id', IntegerType(), True), StructField('dummy_contents', StringType())])\n",
    "db_schema = StructType([StructField('partition_bin', IntegerType(), True), StructField('id', IntegerType(), True), StructField('dummy_contents', StringType(), True), StructField('dea_record_start_date', IntegerType(), True), StructField('dea_record_end_date', IntegerType(), True)])\n",
    "\n",
    "# Day one we get our first set of data\n",
    "data_day_1 = [\n",
    "    (1, 'a'),\n",
    "    (2, 'a'),\n",
    "    (3, 'a'),\n",
    "    (4, 'a'),\n",
    "    (5, 'a'),\n",
    "    (6, 'a'),\n",
    "    (7, 'a'),\n",
    "    (8, 'a'),\n",
    "    (9, 'a')]\n",
    "df1 = spark.createDataFrame(data_day_1, in_schema)\n",
    "df1 = df1.withColumn('partition_bin', F.floor(df1['id']/5))\n",
    "df1.createOrReplaceTempView('df1')\n",
    "\n",
    "# Data 2 we mostly get new data with some old data and updated records\n",
    "data_day_2 = [\n",
    "    (9, 'a'),\n",
    "    (9, 'b'),\n",
    "    (10, 'a'),\n",
    "    (11, 'a'),\n",
    "    (12, 'a'),\n",
    "    (13, 'a')]\n",
    "df2 = spark.createDataFrame(data_day_2, in_schema)\n",
    "df2 = df2.withColumn('partition_bin', F.floor(df2['id']/5))\n",
    "df2.createOrReplaceTempView('df2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Database(name='default', description='Default Hive database', locationUri='file:/home/jovyan/Documents/projects/spark_testing/spark-warehouse')]\n",
      "[Table(name='hive_df', database='default', description=None, tableType='EXTERNAL', isTemporary=False), Table(name='non_hive_df', database='default', description=None, tableType='EXTERNAL', isTemporary=False), Table(name='df1', database=None, description=None, tableType='TEMPORARY', isTemporary=True), Table(name='df2', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]\n"
     ]
    }
   ],
   "source": [
    "print(spark.catalog.listDatabases())\n",
    "print(spark.catalog.listTables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a table that enables HIVE\n",
    "\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS df_test\n",
    "USING HIVE OPTIONS(fileFormat 'PARQUET')\n",
    "PARTITIONED BY (partition_bin)\n",
    "LOCATION 'data/df_test'\n",
    "AS SELECT * FROM df1\n",
    "\"\"\")\n",
    "\n",
    "# Insert df into parition static partition insert\n",
    "spark.sql(\"\"\"\n",
    "INSERT INTO df_test PARTITION (partition_bin = 0)\n",
    "SELECT id, dummy_contents FROM df1 where partition_bin = 0\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+-------------+\n",
      "| id|dummy_contents|partition_bin|\n",
      "+---+--------------+-------------+\n",
      "|  7|             a|            1|\n",
      "|  8|             a|            1|\n",
      "|  9|             a|            1|\n",
      "|  1|             a|            0|\n",
      "|  2|             a|            0|\n",
      "|  3|             a|            0|\n",
      "|  4|             a|            0|\n",
      "|  1|             a|            0|\n",
      "|  2|             a|            0|\n",
      "|  3|             a|            0|\n",
      "|  4|             a|            0|\n",
      "|  3|             a|            0|\n",
      "|  4|             a|            0|\n",
      "|  1|             a|            0|\n",
      "|  2|             a|            0|\n",
      "|  5|             a|            1|\n",
      "|  6|             a|            1|\n",
      "|  9|             a|            1|\n",
      "|  9|             b|            1|\n",
      "+---+--------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM df_test\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Insert overwrite to static partion \n",
    "## Basically we have just inserted df1 into df_test\n",
    "spark.sql(\"\"\"\n",
    "INSERT OVERWRITE TABLE df_test PARTITION (partition_bin = 1)\n",
    "SELECT id, dummy_contents FROM df1 where partition_bin = 1\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+-------------+\n",
      "| id|dummy_contents|partition_bin|\n",
      "+---+--------------+-------------+\n",
      "|  7|             a|            1|\n",
      "|  8|             a|            1|\n",
      "|  9|             a|            1|\n",
      "|  1|             a|            0|\n",
      "|  2|             a|            0|\n",
      "|  3|             a|            0|\n",
      "|  4|             a|            0|\n",
      "|  1|             a|            0|\n",
      "|  2|             a|            0|\n",
      "|  3|             a|            0|\n",
      "|  4|             a|            0|\n",
      "|  3|             a|            0|\n",
      "|  4|             a|            0|\n",
      "|  1|             a|            0|\n",
      "|  2|             a|            0|\n",
      "|  5|             a|            1|\n",
      "|  6|             a|            1|\n",
      "+---+--------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM df_test\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Dynamic partition overwrite (as table was defined as HIVE)\n",
    "# Only overwrites partition_bin = 1 or 2 (as these are the only values in df2)\n",
    "# partition with partition_bin = 0 remains unchanged\n",
    "spark.sql(\"\"\"\n",
    "INSERT OVERWRITE TABLE df_test PARTITION (partition_bin)\n",
    "SELECT id, dummy_contents, partition_bin FROM df2\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+-------------+\n",
      "| id|dummy_contents|partition_bin|\n",
      "+---+--------------+-------------+\n",
      "|  1|             a|            0|\n",
      "|  2|             a|            0|\n",
      "|  3|             a|            0|\n",
      "|  4|             a|            0|\n",
      "|  1|             a|            0|\n",
      "|  2|             a|            0|\n",
      "|  3|             a|            0|\n",
      "|  4|             a|            0|\n",
      "|  3|             a|            0|\n",
      "|  4|             a|            0|\n",
      "|  1|             a|            0|\n",
      "|  2|             a|            0|\n",
      "| 12|             a|            2|\n",
      "| 13|             a|            2|\n",
      "| 12|             a|            2|\n",
      "| 13|             a|            2|\n",
      "|  9|             a|            1|\n",
      "|  9|             b|            1|\n",
      "| 11|             a|            2|\n",
      "| 11|             a|            2|\n",
      "+---+--------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM df_test\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
