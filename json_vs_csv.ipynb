{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!conda install --yes --prefix {sys.prefix} boto3\n",
    "!{sys.executable} -m pip install git+git://github.com/moj-analytical-services/etl_manager.git#egg=etl_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove metastore and data used in this example (Want to start a blank slate)\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "if os.path.isdir('metastore_db') :\n",
    "    shutil.rmtree('metastore_db')\n",
    "\n",
    "if os.path.isfile('derby.log') :\n",
    "    os.remove('derby.log')\n",
    "    \n",
    "if os.path.isdir('data/hive_df') :\n",
    "    shutil.rmtree('data/hive_df')\n",
    "\n",
    "if os.path.isdir('data/non_hive_df') :\n",
    "    shutil.rmtree('data/non_hive_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jovyan/Documents/projects/spark_testing'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Attempt to combine the two\n",
    "\n",
    "import boto3\n",
    "import os\n",
    "# from dataengineeringutils.utils import read_json\n",
    "\n",
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "\n",
    "# Import own function library\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.amazonaws:aws-java-sdk:1.10.34,org.apache.hadoop:hadoop-aws:2.6.0 pyspark-shell'\n",
    "\n",
    "session = boto3.Session()\n",
    "credentials = session.get_credentials()\n",
    "\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"hive.exec.dynamic.partition\", \"true\") \\\n",
    "    .config(\"hive.exec.dynamic.partition.mode\", \"nonstrict\") \\\n",
    "    .config(\"spark.hadoop.fs.s3.impl\", \"org.apache.hadoop.fs.s3native.NativeS3FileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3.awsAccessKeyId\", credentials.access_key) \\\n",
    "    .config(\"spark.hadoop.fs.s3.awsSecretAccessKey\", credentials.secret_key) \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- carat: string (nullable = true)\n",
      " |-- cut: string (nullable = true)\n",
      " |-- color: string (nullable = true)\n",
      " |-- clarity: string (nullable = true)\n",
      " |-- depth: string (nullable = true)\n",
      " |-- table: string (nullable = true)\n",
      " |-- price: string (nullable = true)\n",
      " |-- x: string (nullable = true)\n",
      " |-- y: string (nullable = true)\n",
      " |-- z: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read in test data\n",
    "diamonds = spark.read.csv('data/diamonds/', mode=\"FAILFAST\", header = True)\n",
    "diamonds.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_schema = diamonds.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CHANGING SCHEMA TESTS\n",
    "### missing cols\n",
    "Write data into partitions (as csvs and jsons) (p(0) has all cols, p(1) is missing z column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = 'data/lake/'\n",
    "outpaths = {}\n",
    "for file_type in ['csv', 'parquet', 'json'] :\n",
    "    outpaths[file_type] = base_dir+ f\"{file_type}/\"\n",
    "    diamonds.write.mode('overwrite').format(file_type).save(outpaths[file_type] + \"p=0/\")\n",
    "    diamonds.drop('z').write.mode('overwrite').format(file_type).save(outpaths[file_type] + \"p=1/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try to read over multiple partitions with csv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- carat: string (nullable = true)\n",
      " |-- cut: string (nullable = true)\n",
      " |-- color: string (nullable = true)\n",
      " |-- clarity: string (nullable = true)\n",
      " |-- depth: string (nullable = true)\n",
      " |-- table: string (nullable = true)\n",
      " |-- price: string (nullable = true)\n",
      " |-- x: string (nullable = true)\n",
      " |-- y: string (nullable = true)\n",
      " |-- z: string (nullable = true)\n",
      " |-- p: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_col_test_csv = spark.read.csv(outpaths['csv'], mode=\"FAILFAST\", schema=d_schema)\n",
    "df_col_test_csv.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed to read csv\n"
     ]
    }
   ],
   "source": [
    "from py4j.protocol import Py4JJavaError\n",
    "\n",
    "try :\n",
    "    df_col_test_csv.filter('p=1').show()\n",
    "except Py4JJavaError :\n",
    "    print(\"failed to read csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try to read over multiple partitions with json**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- carat: string (nullable = true)\n",
      " |-- cut: string (nullable = true)\n",
      " |-- color: string (nullable = true)\n",
      " |-- clarity: string (nullable = true)\n",
      " |-- depth: string (nullable = true)\n",
      " |-- table: string (nullable = true)\n",
      " |-- price: string (nullable = true)\n",
      " |-- x: string (nullable = true)\n",
      " |-- y: string (nullable = true)\n",
      " |-- z: string (nullable = true)\n",
      " |-- p: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_col_test_json = spark.read.json(outpaths['json'], mode=\"FAILFAST\", schema=d_schema)\n",
    "df_col_test_json.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+-----+-------+-----+-----+-----+----+----+----+---+\n",
      "|carat|      cut|color|clarity|depth|table|price|   x|   y|   z|  p|\n",
      "+-----+---------+-----+-------+-----+-----+-----+----+----+----+---+\n",
      "| 0.23|    Ideal|    E|    SI2| 61.5|   55|  326|3.95|3.98|null|  1|\n",
      "| 0.21|  Premium|    E|    SI1| 59.8|   61|  326|3.89|3.84|null|  1|\n",
      "| 0.23|     Good|    E|    VS1| 56.9|   65|  327|4.05|4.07|null|  1|\n",
      "| 0.29|  Premium|    I|    VS2| 62.4|   58|  334| 4.2|4.23|null|  1|\n",
      "| 0.31|     Good|    J|    SI2| 63.3|   58|  335|4.34|4.35|null|  1|\n",
      "| 0.24|Very Good|    J|   VVS2| 62.8|   57|  336|3.94|3.96|null|  1|\n",
      "| 0.24|Very Good|    I|   VVS1| 62.3|   57|  336|3.95|3.98|null|  1|\n",
      "| 0.26|Very Good|    H|    SI1| 61.9|   55|  337|4.07|4.11|null|  1|\n",
      "| 0.22|     Fair|    E|    VS2| 65.1|   61|  337|3.87|3.78|null|  1|\n",
      "| 0.23|Very Good|    H|    VS1| 59.4|   61|  338|   4|4.05|null|  1|\n",
      "|  0.3|     Good|    J|    SI1|   64|   55|  339|4.25|4.28|null|  1|\n",
      "| 0.23|    Ideal|    J|    VS1| 62.8|   56|  340|3.93| 3.9|null|  1|\n",
      "| 0.22|  Premium|    F|    SI1| 60.4|   61|  342|3.88|3.84|null|  1|\n",
      "| 0.31|    Ideal|    J|    SI2| 62.2|   54|  344|4.35|4.37|null|  1|\n",
      "|  0.2|  Premium|    E|    SI2| 60.2|   62|  345|3.79|3.75|null|  1|\n",
      "| 0.32|  Premium|    E|     I1| 60.9|   58|  345|4.38|4.42|null|  1|\n",
      "|  0.3|    Ideal|    I|    SI2|   62|   54|  348|4.31|4.34|null|  1|\n",
      "|  0.3|     Good|    J|    SI1| 63.4|   54|  351|4.23|4.29|null|  1|\n",
      "|  0.3|     Good|    J|    SI1| 63.8|   56|  351|4.23|4.26|null|  1|\n",
      "|  0.3|Very Good|    J|    SI1| 62.7|   59|  351|4.21|4.27|null|  1|\n",
      "+-----+---------+-----+-------+-----+-----+-----+----+----+----+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try :\n",
    "    df_col_test_json.filter('p=1').show()\n",
    "except Py4JJavaError :\n",
    "    print(\"failed to read json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try to read over multiple partitions with parquet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- carat: string (nullable = true)\n",
      " |-- cut: string (nullable = true)\n",
      " |-- color: string (nullable = true)\n",
      " |-- clarity: string (nullable = true)\n",
      " |-- depth: string (nullable = true)\n",
      " |-- table: string (nullable = true)\n",
      " |-- price: string (nullable = true)\n",
      " |-- x: string (nullable = true)\n",
      " |-- y: string (nullable = true)\n",
      " |-- z: string (nullable = true)\n",
      " |-- p: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_col_test_parquet = spark.read.parquet(outpaths['parquet'])\n",
    "df_col_test_parquet.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+-----+-------+-----+-----+-----+----+----+----+---+\n",
      "|carat|      cut|color|clarity|depth|table|price|   x|   y|   z|  p|\n",
      "+-----+---------+-----+-------+-----+-----+-----+----+----+----+---+\n",
      "| 0.23|    Ideal|    E|    SI2| 61.5|   55|  326|3.95|3.98|null|  1|\n",
      "| 0.21|  Premium|    E|    SI1| 59.8|   61|  326|3.89|3.84|null|  1|\n",
      "| 0.23|     Good|    E|    VS1| 56.9|   65|  327|4.05|4.07|null|  1|\n",
      "| 0.29|  Premium|    I|    VS2| 62.4|   58|  334| 4.2|4.23|null|  1|\n",
      "| 0.31|     Good|    J|    SI2| 63.3|   58|  335|4.34|4.35|null|  1|\n",
      "| 0.24|Very Good|    J|   VVS2| 62.8|   57|  336|3.94|3.96|null|  1|\n",
      "| 0.24|Very Good|    I|   VVS1| 62.3|   57|  336|3.95|3.98|null|  1|\n",
      "| 0.26|Very Good|    H|    SI1| 61.9|   55|  337|4.07|4.11|null|  1|\n",
      "| 0.22|     Fair|    E|    VS2| 65.1|   61|  337|3.87|3.78|null|  1|\n",
      "| 0.23|Very Good|    H|    VS1| 59.4|   61|  338|   4|4.05|null|  1|\n",
      "|  0.3|     Good|    J|    SI1|   64|   55|  339|4.25|4.28|null|  1|\n",
      "| 0.23|    Ideal|    J|    VS1| 62.8|   56|  340|3.93| 3.9|null|  1|\n",
      "| 0.22|  Premium|    F|    SI1| 60.4|   61|  342|3.88|3.84|null|  1|\n",
      "| 0.31|    Ideal|    J|    SI2| 62.2|   54|  344|4.35|4.37|null|  1|\n",
      "|  0.2|  Premium|    E|    SI2| 60.2|   62|  345|3.79|3.75|null|  1|\n",
      "| 0.32|  Premium|    E|     I1| 60.9|   58|  345|4.38|4.42|null|  1|\n",
      "|  0.3|    Ideal|    I|    SI2|   62|   54|  348|4.31|4.34|null|  1|\n",
      "|  0.3|     Good|    J|    SI1| 63.4|   54|  351|4.23|4.29|null|  1|\n",
      "|  0.3|     Good|    J|    SI1| 63.8|   56|  351|4.23|4.26|null|  1|\n",
      "|  0.3|Very Good|    J|    SI1| 62.7|   59|  351|4.21|4.27|null|  1|\n",
      "+-----+---------+-----+-------+-----+-----+-----+----+----+----+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try :\n",
    "    df_col_test_parquet.filter('p=1').show()\n",
    "except Py4JJavaError :\n",
    "    print(\"failed to read parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reordered cols\n",
    "Write data into partitions (as csvs and jsons) (p(0) has current order, p(1) has cols \"cut\" and \"price\" in different orders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = 'data/lake/'\n",
    "outpaths = {}\n",
    "d_reordered_cols = diamonds.columns\n",
    "d_reordered_cols[1] = 'price'\n",
    "d_reordered_cols[6] = 'cut'\n",
    "for file_type in ['csv', 'parquet', 'json'] :\n",
    "    outpaths[file_type] = base_dir+ f\"{file_type}/\"\n",
    "    diamonds.write.mode('overwrite').format(file_type).save(outpaths[file_type] + \"p=0/\")\n",
    "    diamonds.select(*d_reordered_cols).write.mode('overwrite').format(file_type).save(outpaths[file_type] + \"p=1/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try to read over multiple partitions with csv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- carat: string (nullable = true)\n",
      " |-- cut: string (nullable = true)\n",
      " |-- color: string (nullable = true)\n",
      " |-- clarity: string (nullable = true)\n",
      " |-- depth: string (nullable = true)\n",
      " |-- table: string (nullable = true)\n",
      " |-- price: string (nullable = true)\n",
      " |-- x: string (nullable = true)\n",
      " |-- y: string (nullable = true)\n",
      " |-- z: string (nullable = true)\n",
      " |-- p: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_col_test_csv = spark.read.csv(outpaths['csv'], mode=\"FAILFAST\", schema=d_schema)\n",
    "df_col_test_csv.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "| cut|\n",
      "+----+\n",
      "|2904|\n",
      "|3210|\n",
      "|3414|\n",
      "|3606|\n",
      "|3959|\n",
      "|4032|\n",
      "|4821|\n",
      "|4937|\n",
      "|5325|\n",
      "|5645|\n",
      "|5925|\n",
      "|6194|\n",
      "|6240|\n",
      "|6613|\n",
      "|6731|\n",
      "|7273|\n",
      "|7711|\n",
      "|7762|\n",
      "|9009|\n",
      "|9030|\n",
      "+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from py4j.protocol import Py4JJavaError\n",
    "\n",
    "try :\n",
    "    df_col_test_csv.select(\"cut\").distinct().show()\n",
    "except Py4JJavaError :\n",
    "    print(\"failed to read csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":point_up: Note csv reads ok but obvs cannot tell difference between col mixup because all cols in schema is a string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try to read over multiple partitions with json**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- carat: string (nullable = true)\n",
      " |-- cut: string (nullable = true)\n",
      " |-- color: string (nullable = true)\n",
      " |-- clarity: string (nullable = true)\n",
      " |-- depth: string (nullable = true)\n",
      " |-- table: string (nullable = true)\n",
      " |-- price: string (nullable = true)\n",
      " |-- x: string (nullable = true)\n",
      " |-- y: string (nullable = true)\n",
      " |-- z: string (nullable = true)\n",
      " |-- p: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_col_test_json = spark.read.json(outpaths['json'], mode=\"FAILFAST\", schema=d_schema)\n",
    "df_col_test_json.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|      cut|\n",
      "+---------+\n",
      "|  Premium|\n",
      "|    Ideal|\n",
      "|     Good|\n",
      "|     Fair|\n",
      "|Very Good|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try :\n",
    "    df_col_test_json.select(\"cut\").distinct().show()\n",
    "except Py4JJavaError :\n",
    "    print(\"failed to read json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try to read over multiple partitions with parquet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- carat: string (nullable = true)\n",
      " |-- cut: string (nullable = true)\n",
      " |-- color: string (nullable = true)\n",
      " |-- clarity: string (nullable = true)\n",
      " |-- depth: string (nullable = true)\n",
      " |-- table: string (nullable = true)\n",
      " |-- price: string (nullable = true)\n",
      " |-- x: string (nullable = true)\n",
      " |-- y: string (nullable = true)\n",
      " |-- z: string (nullable = true)\n",
      " |-- p: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_col_test_parquet = spark.read.parquet(outpaths['parquet'])\n",
    "df_col_test_parquet.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|      cut|\n",
      "+---------+\n",
      "|  Premium|\n",
      "|    Ideal|\n",
      "|     Good|\n",
      "|     Fair|\n",
      "|Very Good|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try :\n",
    "    df_col_test_parquet.select(\"cut\").distinct().show()\n",
    "except Py4JJavaError :\n",
    "    print(\"failed to read parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
